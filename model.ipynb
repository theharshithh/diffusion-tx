{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-4\n",
    "NUM_CLASSES = 10 #10 classes to classify\n",
    "PATCH_SIZE=4 \n",
    "IMG_SIZE=28\n",
    "IN_CHANNELS=1\n",
    "NO_OF_HEADS=8\n",
    "DROPOUT=0.001\n",
    "HIDDEN_DIM=768 \n",
    "ADAM_WEIGHT_DECAY=0.1\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION=\"gelu\"\n",
    "NUM_ENCODERS=4\n",
    "EMBED_DIM = (PATCH_SIZE**2) * IN_CHANNELS # 4^2*1=16\n",
    "NUM_PATCHES = (IMG_SIZE//PATCH_SIZE)**2 # 28/4=7; 7^2=49\n",
    "BATCH_SIZE=512\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 50, 16])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super().__init__()\n",
    "        # (b,n_channel,h,w) #(512,n_channel,28,28)\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size), nn.Flatten(2)\n",
    "        )\n",
    "        # op size = (input-kernel)/stride +1 => ((28-4)\\2)+1=7 --> (512,1,7,7)\n",
    "        #nn.Flatten(2) -> flattens last 2 dims. from B,1,H,W -> B,E,H*W\n",
    "        # before flatten - (512, 1, 7,7) after --> (512,16,49)\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b,n_channel,height, width) ip\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # (1, 1, embed_dim) -> (b,1,16)\n",
    "        # x -> (b,c,h,w) -> (b,embed_dim,op_size)\n",
    "\n",
    "        # (512, 1, 28, 28) -> (512,16, 7*7)\n",
    "        x=self.patcher(x)\n",
    "        # exchanges 1,2 idx -> (512,49,embed_dim)\n",
    "        x=x.permute(0,2,1)\n",
    "        \n",
    "        #cls token + x-> (1,1,embed_dim) + (512,49,embed_dim) -> (512, 50, embed_dim)\n",
    "        x=torch.cat(\n",
    "            [cls_token, x], dim=1\n",
    "        )\n",
    "        x+=self.position_embeddings\n",
    "        x= self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "embd_model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)\n",
    "x=torch.randn(512, 1, 28, 28) # b, c, height, width\n",
    "print(embd_model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshith/DL/tx/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_patches, num_classes, patch_size, embed_dim, num_encoders, num_heads, dropout, activation, in_channels):\n",
    "        super().__init__()\n",
    "        self.embedding_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_encoders)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), \n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (ip -> b,channel, h,w)\n",
    "        x=self.embedding_block(x) # (512, total_patches+1, embed_dim)\n",
    "        x=self.encoder_block(x) # (512, total_patches+1, embed_dim)\n",
    "        x=self.mlp_head(x[:,0, :]) # (512, embed_dim) --> [512,10]\n",
    "        return x #(512,10)\n",
    "    \n",
    "model = VIT(NUM_PATCHES, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_ENCODERS, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)\n",
    "x=torch.randn(512, 1,28,28)\n",
    "x=model(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshith/DL/tx/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset   = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader    = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model=VIT(NUM_PATCHES, NUM_CLASSES, PATCH_SIZE,EMBED_DIM,NUM_ENCODERS,NO_OF_HEADS,DROPOUT,ACTIVATION,IN_CHANNELS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser=torch.optim.Adam(model.parameters(), LEARNING_RATE, ADAM_BETAS, weight_decay=ADAM_WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):\n",
    "    print('Training on ', device)\n",
    "    print(f'Epoch: {epoch}')\n",
    "    \n",
    "    model.train()\n",
    "    running_loss =0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        outputs=model(images)\n",
    "        loss=loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss+=loss.item()\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print('Epoch Loss: ', epoch_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
